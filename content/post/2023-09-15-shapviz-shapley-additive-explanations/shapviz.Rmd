---
title: Model interpretability and Shapley Additive Explanations (feat. shapviz)
author: Youbeen Shim
date: '2023-09-15'
slug: shapviz-shapley-additive-explanations
categories: []
tags: []
---

As a modeler, it is often tempting to use the most cutting-edge model that gives the best performance. This is fine as long as the model is properly supported by backtesting, updates, and regular data quality assurance checks. However, the situation changes when working with business stakeholders and decision makers. Issue is this: **the more sophisticated the models become, the harder they are to explain**.

This was the same problem that I ran into when building a model to predict the sale price of homes in the San Francisco & Bay Area. I built an optimized model leveraging unconventional data sources that I gathered through my brother’s web scraping tool, but I was stuck when I tried to answer a simple question from him: “This house looks like the best option for us, but why exactly did your model predict this house would sell for $50,000 above the neighborhood average? Zillow’s proprietary model doesn’t seem to agree with you.”

This is a <abbr title="mostly because he didn’t actually ask it, I made it up for the sake of the narrative">great question</abbr> for today. How do we navigate between the growing tension between model complexity and interpretability? One answer: Shapley Values. 

## Understanding SHAP Values

Let’s start with an analogy that I often use when I am called to explain SHAP values. Imagine this: a team of real estate agents worked tirelessly to sell a house; at the end of the month, they gather to divide their commission - some did more showings, others did more paperwork, and few were dedicated to negotiations with buyers and sellers. How do we fairly attribute the success of the sale to each agent’s contribution?

This is the crux of what SHAP values do. Just as we humans want to fairly distribute credit among our work, SHAP values help us understand how each feature contributes to a model’s prediction. 

While SHAP values get a lot of use in the machine learning space, the mathematical foundation comes from [cooperative game theory](http://www.coalitiontheory.net/research-areas/cooperative-game-theory). If you are so <abbr title="and because I want to show off my LaTeX skills whenever I get the chance">mathematically inclined</abbr>, the Shapley value for feature $i$ is defined as:
$$ \phi_i = \sum_{S \subseteq F \setminus {i}} \frac{|S|!(|F|-|S|-1)!}{|F|!}[f_x(S \cup {i}) - f_x(S)] $$
where:

* $F$ is the set of all features
* $S$ is a subset of features
* $f_x(S)$ is the model's prediction using only features in set $S$

But if you aren’t so keen on math, don't let the formula intimidate you. Just know this, SHAP values:

1. Add up to the difference between the model's prediction and the average prediction
2. Are consistent and fair in attributing feature importance
3. Have solid theoretical foundations

## Implementation

Now, let’s solidify our understanding with R. I’ll be using the Boston housing dataset from the tidyverse package as an example in lieu of my (brother's) proprietary data. 

```{r, message = FALSE, warming = FALSE}
library(shapviz)
library(xgboost)
library(tidyverse)

# using boston housing data
data(Boston, package = "MASS")
X <- as.matrix(Boston[, -14])  # X has all features except the median value
y <- Boston$medv              # target (y) = median value

model <- xgboost(
  data = X,
  label = y,
  nrounds = 100,
  objective = "reg:squarederror",
  verbose = 0
)

# calculate SHAP values
shap <- shapviz(model, X_pred = X)
```

The code is really that simple. The beauty of shapviz is how it simplifies the process of generating and visualizing SHAP values. Now, for visualizations:

First, feature importance. Much like feature importance and permutation importance in other tree based machine learning algorithms, SHAP also has the ability to generate feature importance plots.

```{r}
sv_importance(shap, kind = "bar")
```

However, feature importance still carries its key weakness: it can show how important a single feature is, but it does not show the relationship between the feature and the target. This is where SHAP’s beeswarm summary plot shines. 

```{r}
sv_importance(shap, kind = "beeswarm")
```

Finally, by using the sv_dependence() function, you can create a scatterplot of the SHAP values of a feature against its feature values. Note that, by default, the feature on the color scale is selected via SHAP interactions (if available) or an [interaction heuristic](https://rdrr.io/cran/shapviz/src/R/sv_dependence.R).

```{r}
interesting_xvars <- c("lstat", "rm", "dis", "age")
sv_dependence(shap, v = interesting_xvars)
```


## Back to the original question

Similar to the question that my brother asked, “why exactly did your model predict this house would sell for $50,000 above the neighborhood average?” stakeholders often benefit from understanding how certain model predictions came to be. The shapviz package helps us decompose single predictions with either a waterfall or force plot:

```{r}
interesting_house <- 45  # a house with surprising prediction
sv_waterfall(shap, row_id = interesting_house)
sv_force(shap, row_id = interesting_house)
```
These charts show that, while the model predicted a price $50,000 above average, it wasn’t just because of location (lstat) or size (rm). The interaction between these features tell a more nuanced story.

## Things you should worry about

After using SHAP values to back up my models, there are some key lessons I want to share:

1. Computational Cost: For very large datasets, calculating SHAP values for every prediction can be <abbr title="throwback to my early days when I would wait 3+ hours waiting for R to finish">computationally expensive</abbr>. A good practice is to sample a representative subset for [global](https://www.blog.trainindata.com/machine-learning-interpretability/) [interpretability](https://arxiv.org/html/2406.02981v1) analysis.
2. Feature Independence: SHAP values assume feature independence, and as you already know, this is not always true in real-world data. In our housing example, square footage and number of rooms are clearly correlated. It is important to mitigate this, or at least explain to stakeholders to keep this in mind as they digest your outputs.
3. Interpretation Challenges: While SHAP values are mathematically sound, explaining them to non-technical stakeholders requires careful framing. I’ve found that, just like what I did in this blogpost, starting with concrete examples helps to build intuition. 


## Afterthought
### Why spend time on interpretability?

It is easy for model interpretability to be an afterthought. I must admit that I have often fallen into the pitfall of looking into model interpretability methods after I created a model that I am satisfied with to meet regulatory requirements or stakeholder curiosity. In reality, model interpretability is about building trust in our models and allowing us to make better decisions. SHAP values, especially when combined with key visualizations with tools like shapviz, provide a powerful framework for understanding model predictions. 

In my experience, I’ve found that the time invested in proper model interpretation often pays dividends in stakeholder trust and model adoption. After all, all models are inaccurate, some models are just more useful than others - and **a model is only as good as our ability to understand and explain its decisions**. 

## Reference
### This is great, but I want a better overview (please)

Look into this fantastic write-up that [introduces SHAP by Scott M. Lundberg and Su-In Lee](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf). As well as this overview [“Using ‘shapviz’”](https://cran.r-project.org/web/packages/shapviz/vignettes/basic_use.html) by the shapviz package author Michael Mayer. 



