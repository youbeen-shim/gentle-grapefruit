---
title: Seeing Endogeneity
author: Youbeen Shim
date: '2024-11-23'
slug: seeing-endogeneity
categories: []
tags: []
---

## Endogeneity: Seeing is Believing

Contrary to what I’ve just said in the title, I do not think that you need to see to believe - it sure does help though.

In <abbr title="I hope to do less of multi-series work, since I do not think that it is very conducive to learning.">Part 1</abbr>, we looked at how endogeneity can undermine models; mostly in secret, too. Today, I want to dive into solutions, using simulation to understand the problem and instrumental variables to solve it. I’ll be using a generalization of my work tackling endogeneity in production credit risk models.


## Previously on <abbr title="i was going for a tv series-ish feeling, did it work? Maybe I’m just too much of a nerd">*Endogeneity*...</abbr>

Remember the <abbr title="if you just now visiting us, i dont think you need to read the first part if you already know endogeneity is a problem you want to solve">credit risk model problem</abbr> that I designed for new hires? Interest rates seemed like a perfect predictor of credit risk. However, it was a trap as interest rates and default risk was endogenous. Today, let’s see exactly how this affects our models. 

### Simulating Endogeneity

The credit risk assessment deals with a classic case of endogeneity of *omitted variables*, where an unmeasured variable influences both predictor and the target. Both the predictor, *interest_rate*, and the target, *default_prob*, are influenced by the unobserved *credit_worthiness*. 

<details><summary>click here for the complete simulation code</summary>
```{r}
1+1
```

</details>

First, notice how *credit_worthiness* variable is structured. It follows a standard normal distribution, representing the true quality of each individual that we cannot directly measure in practice. 

```{r, eval=FALSE}
credit_worthiness <- rnorm(n, mean = 0, sd = 1)
```

Then, we explicitly design endogeneity into the relationship. Notice how the formula creates a <abbr title="the exponential term creates a more realistic pattern where interest rates have a floor but can increase substantially for risky borrowers.">nonlinear relationship</abbr> where better credit worthiness leads to lower interest rates, plus some random noise. 

```{r, eval=FALSE}
interest_rate = 0.05 + 0.03 * exp(-credit_worthiness) + rnorm(n, 0, 0.01)
```




```{r, echo=FALSE}
# Section 2: Simulating Endogeneity in Credit Risk Assessment

## Load required libraries
library(tidyverse)
library(ggplot2)
library(broom)

## Simulate underlying data structure
set.seed(42)  # For reproducibility

simulate_credit_data <- function(n = 1000) {
  # First, simulate the unobserved credit worthiness
  # We use normal distribution to represent the underlying quality
  credit_worthiness <- rnorm(n, mean = 0, sd = 1)
  
  # Now simulate observable features
  # Note how some variables are influenced by credit_worthiness
  tibble(
    credit_worthiness = credit_worthiness,
    
    # Business characteristics
    business_age = rexp(n, rate = 1/5) + abs(credit_worthiness),  # Older businesses tend to have better credit
    annual_revenue = exp(rnorm(n, 11, 1) + 0.5 * credit_worthiness),  # Revenue partially reflects creditworthiness
    
    # Financial metrics
    current_debt = exp(rnorm(n, 10, 0.8) - 0.3 * credit_worthiness),  # Better credit worthiness → less debt
    cash_flow_volatility = rexp(n, rate = 2) - 0.2 * credit_worthiness,  # Better credit → more stable
    
    # Endogenous variables
    interest_rate = 0.05 + 0.03 * exp(-credit_worthiness) + rnorm(n, 0, 0.01),
    
    # Credit utilization (ratio of current debt to credit limit)
    credit_utilization = pmin(1, pmax(0, rbeta(n, 2, 4) - 0.3 * credit_worthiness)),
    
    # Target variable: default probability
    default_prob = plogis(-2 - 1.5 * credit_worthiness + 
                         0.2 * cash_flow_volatility +
                         rnorm(n, 0, 0.5))  # Add some noise
  )
}

# Generate our dataset
credit_data <- simulate_credit_data(1000)

# Fit naive model ignoring endogeneity
naive_model <- lm(default_prob ~ business_age + annual_revenue + current_debt + 
                 interest_rate + credit_utilization + cash_flow_volatility,
                 data = credit_data)

# Section 3: Visualizing the Impact of Endogeneity

## Plot 1: Interest Rate vs Default Probability
p1 <- ggplot(credit_data, aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.4, aes(color = credit_worthiness)) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  scale_color_viridis_c() +
  labs(title = "Interest Rate vs Default Probability",
       subtitle = "Color indicates underlying credit worthiness",
       x = "Interest Rate",
       y = "Default Probability",
       color = "Credit\nWorthiness") +
  theme_minimal()

## Plot 2: Credit Worthiness Impact
p2 <- credit_data %>%
  pivot_longer(cols = c(interest_rate, default_prob),
               names_to = "variable",
               values_to = "value") %>%
  ggplot(aes(x = credit_worthiness, y = value)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~variable, scales = "free_y") +
  labs(title = "Impact of Credit Worthiness",
       subtitle = "Showing relationship with endogenous variables",
       x = "Credit Worthiness",
       y = "Variable Value") +
  theme_minimal()

## Plot 3: Coefficient Plot
coef_plot <- tidy(naive_model) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(x = reorder(term, estimate), y = estimate)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  geom_errorbar(aes(ymin = estimate - std.error*1.96,
                    ymax = estimate + std.error*1.96),
                width = 0.2) +
  coord_flip() +
  labs(title = "Naive Model Coefficients",
       subtitle = "Notice the potentially misleading interest rate effect",
       x = "Variable",
       y = "Coefficient Estimate") +
  theme_minimal()

# Display plots
print(p1)
print(p2)
print(coef_plot)

# Calculate correlations to show the endogeneity structure
cor_matrix <- credit_data %>%
  select(credit_worthiness, interest_rate, default_prob) %>%
  cor() %>%
  round(3)

print("Correlation Matrix:")
print(cor_matrix)
```


```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
set.seed(1008)

simulate_loan_data <- function(n_samples = 1000) {
  # True underlying creditworthiness (unobserved)
  creditworthiness <- rnorm(n_samples, mean = 0, sd = 1)
  
  # Generate business characteristics
  business_age <- rpois(n_samples, lambda = 5)
  annual_revenue <- exp(rnorm(n_samples, mean = 11, sd = 1))
  
  # Interest rate depends on creditworthiness and observables
  interest_rate <- 5 + 
                  -0.5 * creditworthiness +  # Higher creditworthiness → Lower rate
                  -0.1 * business_age +
                  -0.1 * log(annual_revenue) +
                  rnorm(n_samples, mean = 0, sd = 0.5)
  
  # Default probability depends on creditworthiness and interest rate
  default_prob <- plogis(-2 +
                        -0.8 * creditworthiness +  # Higher creditworthiness → Lower default
                        0.2 * interest_rate +
                        -0.1 * business_age +
                        -0.1 * log(annual_revenue))
  
  # Realize defaults
  default <- rbinom(n_samples, size = 1, prob = default_prob)
  
  # Create dataset
  data.frame(
    business_age = business_age,
    annual_revenue = annual_revenue,
    interest_rate = interest_rate,
    creditworthiness = creditworthiness,  # Usually unobserved
    default = default
  )
}

# Generate data
loan_data <- simulate_loan_data()
```

```{r}
genEndogData <- function(n=100){
  sigma <- matrix(c(40,12,12,20), ncol=2)
  de <- mvtnorm::rmvnorm(n=n, mean=c(0,0), sigma=sigma)
  Z <- seq(1,5,length.out = n)
  X <- 3*Z + de[,1]
  Y <- 2*X + de[,2] 
  data.frame(Y,X,Z)
}

test_data <- genEndogData()

test_model <- glm(Z ~ X + Y,
                  family = "gaussian",
                  data = test_data)

viz_test_data <- test_data %>%
  mutate(
    predicted_prob = predict(test_model, type = "response"),
    X_group = cut(X, 
                                breaks = quantile(X, probs = 0:5/5),
                                labels = c("Very Low", "Low", "Medium", "High", "Very High"))
  )

# Plot relationships
ggplot(viz_test_data, aes(x = Y, y = Z)) +
  geom_point(alpha = 0.3, aes(color = X_group)) +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  facet_wrap(~X_group) +
  labs(title = "(target: Z) vs (value: Y)",
       subtitle = "Grouped by: X",
       x = "Y (%)",
       y = "Z",
       color = "X") +
  theme_minimal()

```

```{r}
genEndogData <- function(n=100){
  sigma <- matrix(c(40,12,12,20), ncol=2)
  de <- mvtnorm::rmvnorm(n=n, mean=c(0,0), sigma=sigma)
  Z <- seq(1,5,length.out = n)
  X <- 3*Z + de[,1]
  Y <- 2*X + de[,2] 
  data.frame(Y,X,Z)
}

test_data <- genEndogData()

test_model <- glm(Z ~ X + Y,
                  family = "gaussian",
                  data = test_data)

viz_test_data <- test_data %>%
  mutate(
    predicted_prob = predict(test_model, type = "response"),
    Y_group = cut(Y, 
                                breaks = quantile(Y, probs = 0:5/5),
                                labels = c("Very Low", "Low", "Medium", "High", "Very High"))
  )

# Plot relationships
ggplot(viz_test_data, aes(x = X, y = Z)) +
  geom_point(alpha = 0.3, aes(color = Y_group)) +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  facet_wrap(~Y_group) +
  labs(title = "(target: Z) vs (value: X)",
       subtitle = "Grouped by: Y",
       x = "X (%)",
       y = "Z",
       color = "Y") +
  theme_minimal()
```


```{r}
# Fit naive model
naive_model <- glm(default ~ interest_rate + business_age + log(annual_revenue),
                  family = binomial(link = "logit"),
                  data = loan_data)

# Create visualization data
viz_data <- loan_data %>%
  mutate(
    predicted_prob = predict(naive_model, type = "response"),
    creditworthiness_group = cut(creditworthiness, 
                                breaks = quantile(creditworthiness, probs = 0:5/5),
                                labels = c("Very Low", "Low", "Medium", "High", "Very High"))
  )

# Plot relationships
ggplot(viz_data, aes(x = interest_rate, y = default)) +
  geom_point(alpha = 0.3, aes(color = creditworthiness_group)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial")) +
  facet_wrap(~creditworthiness_group) +
  labs(title = "Default Probability vs Interest Rate",
       subtitle = "Grouped by Creditworthiness",
       x = "Interest Rate (%)",
       y = "Default Probability",
       color = "Creditworthiness") +
  theme_minimal()
```

```{r}
# Simulate instruments
simulate_instruments <- function(loan_data, n_samples = nrow(loan_data)) {
  # Federal funds rate (varies by time)
  fed_rate <- rep(runif(n_samples/20, 1, 3), each = 20) + 
              rnorm(n_samples, 0, 0.1)
  
  # Bank liquidity (varies by region)
  bank_liquidity <- rep(rnorm(n_samples/10), each = 10)
  
  # Create instrument dataset
  data.frame(
    fed_rate = fed_rate,
    bank_liquidity = bank_liquidity,
    # Interest rate spread (market-wide)
    market_spread = 2 + 0.5 * fed_rate + rnorm(n_samples, 0, 0.2)
  )
}

# Add instruments to our data
loan_data <- cbind(loan_data, simulate_instruments(loan_data))

# Two-Stage Least Squares (2SLS)
# First stage: predict interest rates using instruments
stage1 <- lm(interest_rate ~ fed_rate + bank_liquidity + market_spread +
             business_age + log(annual_revenue),
             data = loan_data)

# Get predicted interest rates
loan_data$predicted_rate <- predict(stage1)

# Second stage: use predicted rates in default model
stage2 <- glm(default ~ predicted_rate + business_age + log(annual_revenue),
              family = binomial(link = "logit"),
              data = loan_data)

# Compare coefficients
models_comparison <- data.frame(
  Variable = names(coef(naive_model)),
  Naive_Estimate = coef(naive_model),
  IV_Estimate = c(coef(stage2)[1], 
                 coef(stage2)["predicted_rate"],
                 coef(stage2)[c("business_age", "log(annual_revenue)")])
)
```

```{r}
# Function to evaluate model performance
evaluate_model <- function(model, data, predicted_rate = FALSE) {
  # Get predictions
  if(predicted_rate) {
    data$interest_rate <- data$predicted_rate
  }
  predictions <- predict(model, newdata = data, type = "response")
  
  # Calculate metrics
  auc <- pROC::auc(data$default, predictions)
  
  # Calculate calibration metrics
  calibration <- data.frame(
    pred_prob = predictions,
    actual = data$default
  ) %>%
    mutate(bucket = cut(pred_prob, breaks = seq(0, 1, 0.1))) %>%
    group_by(bucket) %>%
    summarise(
      n = n(),
      pred = mean(pred_prob),
      actual = mean(actual)
    )
  
  list(auc = auc, calibration = calibration)
}

# Compare models
naive_perf <- evaluate_model(naive_model, loan_data)
iv_perf <- evaluate_model(stage2, loan_data, predicted_rate = TRUE)
```
```{r}
naive_perf
```
```{r}
iv_perf
```




