---
title: Seeing Endogeneity
author: Youbeen Shim
date: '2024-11-23'
slug: seeing-endogeneity
categories: []
tags: []
---

## Seeing is Believing

Contrary to what I’ve just said, I do not think that you need to see to believe - it sure does help though.

In <abbr title="I hope to do less of multi-series work, since I do not think that it is very conducive to learning.">Part 1</abbr>, we looked at how endogeneity can undermine models; mostly in secret, too. Today, I want to dive into solutions, using simulation to understand the problem and instrumental variables to solve it. I’ll be using a generalization of my work tackling endogeneity in production credit risk models.

Remember the <abbr title="if you just now visiting us, i dont think you need to read the first part if you already know endogeneity is a problem you want to solve">credit risk model problem</abbr> that I designed for new hires? Interest rates seemed like a perfect predictor of credit risk. However, it was a trap as interest rates and default risk was endogenous. Today, let’s see exactly how this affects our models. 

## Simulating Endogeneity

The credit risk assessment deals with a classic case of endogeneity of **omitted variables**, where an unmeasured variable influences both predictor and the target. Both the predictor, *interest_rate*, and the target, *default_prob*, are influenced by the unobserved *credit_worthiness*. 

<details><summary>click here for the packages used in this blog, along with quick descriptions </summary>

You can decide if you want to use my code directly, or fine-tune with packages of your preference.
```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)

library(xgboost) # only if you want to use the xgboost's feature importance
library(AER)  # For instrumental variables regression
library(Metrics) # for the metrics

set.seed(7345)
```
</details>

<details><summary>click here for the complete simulation code</summary>
```{r}
simulate_credit_data <- function(n = 1000) {
  credit_worthiness <- rnorm(n, mean = 0, sd = 1)
  
  tibble(
    credit_worthiness = credit_worthiness,
    
    # Most businesses are young, with decreasing frequency for older ones
    business_age = rexp(n, rate = 1/5),  # mean of ~5 years
    
    # Log-normal distribution for realistic revenue distribution
    annual_revenue = exp(rnorm(n, 11, 1.2)),  # mostly 20k~150k
    
    # Log-normal, mild correlation with revenue
    current_debt = exp(rnorm(n, 10, 0.8)) * (1 + 0.2 * scale(annual_revenue)),
    
    # Higher for smaller businesses
    cash_flow_volatility = rexp(n, rate = 1) * (1 - 0.3 * scale(annual_revenue)),
    
    # Beta distribution for realistic ratio behavior
    credit_utilization = rbeta(n, 2, 4),  # right-skewed, mostly under 0.6
    
    # Endogenous variables
    # interest rate affected by credit worthiness
    interest_rate = 0.05 + 0.03 * exp(-credit_worthiness) + rnorm(n, 0, 0.01),
    
    # default probability affected by credit worthiness and business fundamentals
    default_prob = plogis(-2 - 1.2 * credit_worthiness +  # Credit worthiness effect
                         0.3 * cash_flow_volatility +     # Business risk effect
                         0.2 * credit_utilization +       # Utilization effect
                         rnorm(n, 0, 0.5))                # Random noise
  )
}

# generate data
credit_data <- simulate_credit_data(1000)
```

> Note: If you are the diligent type that is taking note of all the code, you'll notice that I am using ***credit_data_iv*** instead of ***credit_data*** for all of the modeling & visualization code below. Pretend for now that those two objects are exactly the same thing. Why one is in place of the other will make more sense later in the post.

```{r, echo=FALSE}
simulate_credit_data_with_instrument <- function(n = 1000) {
  data <- simulate_credit_data(n) # base simulation
  
  data$regulatory_zone <- sample(1:3, n, replace = TRUE) # our instrument
  
  # mod interest_rate behavior so that it is affected by regulatory zones
  data$interest_rate <- data$interest_rate + 
                        (data$regulatory_zone - 2) * 0.05 +  # regulatory effect
                        rnorm(n, 0, 0.005)  # noise
  
  return(data)
}

credit_data_iv <- simulate_credit_data_with_instrument(1000)
```
</details>

First, notice how *credit_worthiness* variable is structured. It follows a standard normal distribution, representing the true quality of each individual that we cannot directly measure in practice. 

```{r, eval=FALSE}
credit_worthiness <- rnorm(n, mean = 0, sd = 1)
```

Then, we explicitly design endogeneity into the relationship. Notice how the formula creates a <abbr title="the exponential term creates a more realistic pattern where interest rates have a floor but can increase substantially for risky borrowers.">nonlinear relationship</abbr> where better credit worthiness leads to lower interest rates, plus some random noise. 

```{r, eval=FALSE}
interest_rate = 0.05 + 0.03 * exp(-credit_worthiness) + rnorm(n, 0, 0.01)
```


## Seeing Endogeneity

With the data now generated, we have sufficient knowledge that the features have endogeneity. We can now combine typical exploratory data work combined with some tips from the previous post to gauge the impact of endogeneity. In the visualizations below, note how endogeneity manifests in the data.

### Plot of Coefficients

A naive model that ignores endogeneity and uses all of the available data at hand might look something like this:

```{r}
naive_model <- lm(default_prob ~ business_age + annual_revenue + current_debt + 
                                 interest_rate + credit_utilization + cash_flow_volatility,
                  data = credit_data_iv)
```

Using the model above, I can generate a plot of its coefficients. This is typically done to [gauge each individual feature's importance](https://machinelearningmastery.com/calculate-feature-importance-with-python), which is intuitive since [coefficients show how much the target variable changes for a one-unit increase in the feature, assuming all other features remain constant](https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression).

<details><summary>click here for the ggplot code</summary>
```{r}
coef_plot <- broom::tidy(naive_model) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(x = reorder(term, estimate), y = estimate)) +
  geom_col(fill = "blue", alpha = 0.7) +
  geom_errorbar(aes(ymin = estimate - std.error*1.96,
                    ymax = estimate + std.error*1.96),
                width = 0.2) +
  coord_flip() +
  labs(title = "Coefficients from the Naive Model",
       subtitle = "Note the (potentially misleading) interest rate effect",
       x = "Variable",
       y = "Coefficient Estimate") +
  theme_minimal()
```
</details>

```{r, echo=FALSE}
print(coef_plot)
```

If you are knee-deep in the machine learning world, and you *must* see how a naive xgboost might perform, I got you.

<details><summary>click here for the code</summary>
```{r}
# xgboost requires the data to be in a matrix format
features <- c("business_age", "annual_revenue", "current_debt", 
              "interest_rate", "credit_utilization", "cash_flow_volatility")
# Note: credit_worthiness is excluded since it's our unobserved variable
train_matrix <- as.matrix(credit_data_iv[features])
train_label <- credit_data_iv$default_prob

# train 
xgb_model <- xgboost(
  data = train_matrix,
  label = train_label,
  nrounds = 100,
  objective = "reg:squarederror",
  verbose = 0
)

# feature importance
importance <- xgb.importance(
  feature_names = features,
  model = xgb_model
)

importance_plot <- ggplot(importance, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_col(fill = "blue", alpha = 0.7) +
  coord_flip() +
  labs(title = "Feature Importance from the Naive XGBoost Model",
       subtitle = "Measured as contribution to model performance",
       x = "Feature",
       y = "Gain (Importance Score)") +
  theme_minimal()
```
</details>

```{r, echo=FALSE}
print(importance_plot)
```

As you can see, if anything, machine learning algorithms are even more likely to identify *interest_rate* as a strong predictor (which is obvious, given its endogeneity) and overfit on it. This can lead to some disasterous misinterpretation and a lot of head scratching when your models underperform in live testing.  


### Interest Rate vs Default Probability

Observing the overwhelming power of *interest_rate*, you might be inclined to see it's direct relationship with with our target, *default_prob*. Notice how the apparent relationship between the two variables (the red line) can mislead us. While the line shows almost a linear relationship, by incorporating *credit_worthiness*, we can see it has a gradual gradient along the graph implying that both variables could be influenced by the variable that we cannot observe. 

<details><summary>click here for the ggplot code</summary>
```{r}
onetoone <- ggplot(credit_data_iv, aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(title = "Interest Rate vs Default Probability",
       x = "Interest Rate",
       y = "Default Probability",
       color = "Credit\nWorthiness") +
  theme_minimal()
```

```{r}
onetoone_w_credit <- ggplot(credit_data_iv, aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.6, aes(color = credit_worthiness)) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  scale_color_viridis_c() +
  labs(title = "Interest Rate vs Default Probability",
       x = "Interest Rate",
       y = "Default Probability",
       color = "Credit\nWorthiness") +
  theme_minimal()
```
</details>

```{r, figures-side, fig.show="hold", out.width="50%", message=FALSE, warning=FALSE, echo=FALSE}
print(onetoone)
print(onetoone_w_credit)
```

We can drive this point further by breaking the groups down into a quantile using *credit_worthiness*, and graphing the relationship between *default_prob* and *interest_rate* for each quantile, like so:

```{r, eval=FALSE}
credit_worthiness =  cut(credit_worthiness, 
                         breaks = quantile(credit_worthiness, probs = 0:4/4),
                         labels = c("Very Low", "Moderate Low", "Moderate High", "Very High")
```

<details><summary>click here for the complete code</summary>

With uniform scale

```{r}
onetoone_quantile <- credit_data_iv %>%
  mutate(credit_worthiness =  cut(round(credit_worthiness, 8) , 
                                  breaks = quantile(credit_worthiness, probs = 0:4/4),
                                  labels = c("Very Low", "Moderate Low", "Moderate High", "Very High"))
  )

onetoone_quantile_p <- ggplot(onetoone_quantile, aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.3, aes(color = credit_worthiness)) +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  facet_wrap(~credit_worthiness) +
  labs(title = "default_prob vs interest_rate",
       subtitle = "Grouped by credit_worthiness",
       x = "interest_rate (%)",
       y = "default_prob",
       color = "credit_worthiness") +
  theme_minimal()
```

(the version shown in the post) With xlim & ylim of (0, 1.5) for the "very low", and (0, 0.5) for "moderate low", "moderate high", and "very high"

```{r}
p1 <- onetoone_quantile %>% filter(credit_worthiness == "Very Low") %>%
ggplot(aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.4, color ='red') +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  labs(title = "Credit Worthiness: Very Low",
       x = "interest_rate (%)",
       y = "default_prob") +
  xlim(0, 1.5) + 
  ylim(0, 1.5) + 
  theme_minimal()

p2 <- onetoone_quantile %>% filter(credit_worthiness == "Moderate Low") %>%
ggplot(aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.4, color ='green') +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  labs(title = "Credit Worthiness: Moderate Low",
       x = "interest_rate (%)",
       y = "default_prob") +
  xlim(0, 0.5) + 
  ylim(0, 0.5) + 
  theme_minimal()

p3 <- onetoone_quantile %>% filter(credit_worthiness == "Moderate High") %>%
ggplot(aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.4, color ='blue') +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  labs(title = "Credit Worthiness: Moderate Low",
       x = "interest_rate (%)",
       y = "default_prob") +
  xlim(0, 0.5) + 
  ylim(0, 0.5) + 
  theme_minimal()

p4 <- onetoone_quantile %>% filter(credit_worthiness == "Very High") %>%
ggplot(aes(x = interest_rate, y = default_prob)) +
  geom_point(alpha = 0.4, color ='purple') +
  geom_smooth(method = "glm", method.args = list(family = "gaussian")) +
  labs(title = "Credit Worthiness: Very High",
       x = "interest_rate (%)",
       y = "default_prob") +
  xlim(0, 0.5) + 
  ylim(0, 0.5) + 
  theme_minimal()
```
</details>

```{r, figures-quad, fig.show="hold", out.width="50%", message=FALSE, warning=FALSE, echo=FALSE}
print(p1)
print(p2)
print(p3)
print(p4)
```

While a linear-ish relationship can be found, the slope is different for each group.

## Handling Endogeneity - Instrumental Variables

Hopefully by now, I have successfully convinced you that 1) endogeneity is a problem, 2) it is common, and 3) it is often well hidden. Borrowing my undergraduate adviser's words, for a scientist looking to study the world through data plagued by endogeneity, it is [akin to] trying to see your reflection in rippling water. The image you see is distorted by the waves.

In our case, when measuring how interest rates affect default probability, the *waves* of credit_worthiness affect our perception. So what do we do? We design a special kind of mirror. 

### Identifying Instrumental Variables
We find a variable that does the following:

1. directly influences interest rates (Relevance)

2. only effects default probability through its effect on interest rates (Exclusion restriction)

3. is independent of underlying credit worthiness (Exogeneity)

The biggest challenge is identifying the variable that achieves those conditions. My idea was to create a ***regulatory zone*** instrument. In real life, *regulatory_zones* create different regions having slightly different banking regulations that affect interest rates. These regulations don’t directly influence whether a person will default, nor are they related to the person’s underlying credit worthiness. 

In lieu of actual data, I'll add a *regulatory_zone* variable to our baseline simulation, simulating a regulatory change that affects  *interest_rate* but not *default_probability* directly. We'll also ensure that it is independent of *credit_worthiness*.

```{r, eval=FALSE}
simulate_credit_data_with_instrument <- function(n = 1000) {
  data <- simulate_credit_data(n) # base simulation
  
  data$regulatory_zone <- sample(c(0.05, 0.07, 0.1), n, replace = TRUE) # our instrument
  
  # mod interest_rate behavior so that it is affected by regulatory zones
  data$interest_rate <- data$interest_rate + 
                        data$regulatory_zone * 0.1 +  # regulatory effect
                        rnorm(n, 0, 0.005)  # noise
  
  return(data)
}

credit_data_iv <- simulate_credit_data_with_instrument(1000)
```

### Utilizing Instrument Variables

Once the instrument variables are *found*, the methodology, as we covered in the previous post, is just a two-stage least squares. 

First stage: we regress the endogenous variable (*interest_rates*) on our instrument (*regulatory_zone*) and other controls.

```{r}
first_stage <- lm(interest_rate ~ regulatory_zone + business_age + annual_revenue + 
                                  current_debt + credit_utilization + cash_flow_volatility,
                  data = credit_data_iv)
```

Second stage: use these predicted interest rates to estimate their effect on default probability.

```{R}
# ivreg() fn uses the AER package to perform a 2-Stage Least Squares method
iv_model <- ivreg(default_prob ~ business_age + annual_revenue + current_debt + 
                                 interest_rate + credit_utilization + cash_flow_volatility | 
                                 business_age + annual_revenue + current_debt + regulatory_zone + 
                                 credit_utilization + cash_flow_volatility,
                  data = credit_data_iv)
```

## What did IV achieve? 

So what was all that? Let's take it back to the basics. We initially identified a variable, *interest_rate*, which seemed like a promising explanatory variable but turned out to be highly endogeneious with our target. So we found a new variable, called an instrument, *regulatory_zone*, that is correlated with *interest_rate* but not the error term. This instrument helps to isolate the part of the explanatory variable that is not correlated with the error term. In essence, it allows for more accurate estimation of the relationship between the explanatory variable (*interest_rate*) and the dependent variable (*default_prob*).

In order to do so, we used a two-stage least squares (2SLS) technique. In the first stage, *interest_rate* (our problematic variable) is regressed on *regulatory_zone* (instrument) along with other exogenous variables. This creates a new version of the *interest_rate* which is now effectively devoid of its correlation with the error term. Then, the original regression is estimated (with *default_prob* as the dependent variable), but this time using the predicted values from the first stage regression in place of the original *interest_rate*.

Now that we did all that, we can see how things have shifted:

### Plot of Coefficients

<details><summary>click here for the complete code</summary>
```{r}
coef_comparison <- 
  bind_rows(
    broom::tidy(naive_model) %>% filter(term != "(Intercept)") %>% mutate(model = "Naive OLS"),
    broom::tidy(iv_model) %>% filter(term != "(Intercept)") %>% mutate(model = "IV 2SLS")
  ) %>%
  ggplot(aes(x = reorder(term, estimate), y = estimate, fill = model)) +
  geom_col(position = "dodge", alpha = 0.7) +
  geom_errorbar(aes(ymin = estimate - std.error*1.96,
                    ymax = estimate + std.error*1.96),
                position = position_dodge(width = 0.9),
                width = 0.2) +
  coord_flip() +
  scale_fill_manual(values = c("red", "blue")) +
  labs(title = "Coefficient Estimates: Naive vs IV",
       subtitle = "Notice the change in interest rate effect after IV correction",
       x = "Variable",
       y = "Coefficient Estimate",
       fill = "Model Type") +
  theme_minimal()
```

</details>

```{r, message=FALSE, warning=FALSE, echo=FALSE}
print(coef_comparison)
```

The above coefficient comparison plot reveals an obvious and quite striking difference in the effect of *interest_rate* between the naive and IV models. While the naive model shows a moderate positive relationship, the IV model estimates a much larger effect with wider confidence intervals. This difference suggests that the naive model significantly underestimated the true impact of *interest_rate* on *default_prob*. 

Going back to the simulation code
```{r, eval=FALSE}
default_prob = plogis(-2 - 1.2 * credit_worthiness +  
                           0.3 * cash_flow_volatility + 
                           0.2 * credit_utilization +  
                           rnorm(n, 0, 0.5))        
```

we can see that the updated coefficient estimates coming from the IV 2SLS model is a much more grounded, and captures the effect of *interest_rate* on *defaut_prob* more accurately. 

### Interest Rate vs Default Probability

<details><summary>click here for the complete code</summary>

```{r}
relationship_comparison <- 
  ggplot(credit_data_iv, aes(x = interest_rate, y = default_prob)) +
  # naive relationship
  geom_point(alpha = 0.4, aes(color = credit_worthiness)) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  # iv predicted relationship
  geom_smooth(aes(y = predict(iv_model, credit_data_iv)), 
             method = "lm", color = "blue", se = FALSE, linetype = "dashed") +
  scale_color_viridis_c() +
  labs(title = "Interest Rate vs Default Probability; Naive vs IV",
       subtitle = "Blue = Naive relationship, Red = IV-corrected relationship",
       x = "Interest Rate",
       y = "Default Probability",
       color = "Credit\nWorthiness") +
  theme_minimal()
```

</details>

```{r, message=FALSE, warning=FALSE, echo=FALSE}
print(relationship_comparison)
```

In the updated scatter plot above, we can find an even more compelling effect. The naive model (represented by the blue dashed line) suggests a strong linear relationship between interest rates and default probability. However, as we've discussed before, looking at the gradient point colors that represent credit worthiness, we can see this relationship is largely driven by the underlying credit worithness. The IV model (represented by the red solid line) corrects for this endogeneity, showing a more modest slope that likely better represents the true causal relationship between interest rates and default probability. 

### R-squared

Finally, you can observe that the r-squared value of the naive model,
```{r}
summary(naive_model)$r.squared
```

is greater than the r-squared value observed in the IV model,
```{r}
summary(iv_model)$r.squared
```

The decrease in r-squared actually strengthens the validity of the IV model. The naive model's higher r-squared was artificially inflated by capturing the effect of the unobserved credit worthiness. The IV model's lower r-squared reflects that it's isolating just hte causal effect of *interest_rate*, removing the confounding influence of credit worthiness. The IV model resulted in the intended effect of **more accurate measure of the true effect of interest_rate**. 

## Practical Tips

### When implementing, consider…

1. Start simple

The easiest trap to fall into is hastily implementing a complex set of methods that promise to solve everything. Begin with a single, strong, instrument, and only add complexity as needed. It is more important to validate each step thoroughly. 

2. Test more than once

“Testing” here goes beyond checking your instrument strength using first-stage F-statistics. Chances are, you are not a domain expert. Validate exclusion restriction using whatever domain knowledge is available. Finally, clearly explain the IV approach to stakeholders and make sure that there is an institutional-level (or at least team-level) buy-in. 

3. Build robustly

Data intake (data pipelines) is typically the most vulnerable and fragile area. It is also the area that tends to be blindly trusted, this should not be the case. Make sure to handle missing instrument data, create more than one fallback options, and document every assumption check.

4. Monitor

Good intuition will never be as powerful as carefully designed  monitoring. Track instrument strength over time, building especially guarded against how relationships between variables can change. 

In my case, I designed monitoring dashboards and automated alerts that would alert the engineer in charge if the relationship between instruments and interest rates started to behave unexpectedly. 

### A challenge for you

Now that we have covered both the problem and the solution, it’s time that you challenged yourself. Review the models that you or your team is responsible for designing and/or maintaining. Are there any concerns for endogeneity? What are some promising instruments in your domain? Remember to start small, and work towards building a system that can handle comprehensive monitoring for all of your models.
Solving endogeneity isn’t simply about building better models. It is about making better decisions that affect real businesses and people.



