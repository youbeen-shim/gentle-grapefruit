---
title: Seeing Endogeneity
author: Youbeen Shim
date: '2024-11-23'
slug: seeing-endogeneity
categories: []
tags: []
---



<div id="endogeneity-seeing-is-believing" class="section level2">
<h2>Endogeneity: Seeing is Believing</h2>
<p>Contrary to what I’ve just said in the title, I do not think that you need to see to believe - it sure does help though.</p>
<p>In <abbr title="I hope to do less of multi-series work, since I do not think that it is very conducive to learning.">Part 1</abbr>, we looked at how endogeneity can undermine models; mostly in secret, too. Today, I want to dive into solutions, using simulation to understand the problem and instrumental variables to solve it. I’ll be using a generalization of my work tackling endogeneity in production credit risk models.</p>
</div>
<div id="previously-on-endogeneity" class="section level2">
<h2>Previously on <abbr title="i was going for a tv series-ish feeling, did it work? Maybe I’m just too much of a nerd"><em>Endogeneity</em>…</abbr></h2>
<p>Remember the <abbr title="if you just now visiting us, i dont think you need to read the first part if you already know endogeneity is a problem you want to solve">credit risk model problem</abbr> that I designed for new hires? Interest rates seemed like a perfect predictor of credit risk. However, it was a trap as interest rates and default risk was endogenous. Today, let’s see exactly how this affects our models.</p>
<div id="simulating-endogeneity" class="section level3">
<h3>Simulating Endogeneity</h3>
<p>The credit risk assessment deals with a classic case of endogeneity of <em>omitted variables</em>, where an unmeasured variable influences both predictor and the target. Both the predictor, <em>interest_rate</em>, and the target, <em>default_prob</em>, are influenced by the unobserved <em>credit_worthiness</em>.</p>
<details>
<summary>
click here for the complete simulation code
</summary>
<pre class="r"><code>1+1</code></pre>
<pre><code>## [1] 2</code></pre>
</details>
<p>First, notice how <em>credit_worthiness</em> variable is structured. It follows a standard normal distribution, representing the true quality of each individual that we cannot directly measure in practice.</p>
<pre class="r"><code>credit_worthiness &lt;- rnorm(n, mean = 0, sd = 1)</code></pre>
<p>Then, we explicitly design endogeneity into the relationship. Notice how the formula creates a <abbr title="the exponential term creates a more realistic pattern where interest rates have a floor but can increase substantially for risky borrowers.">nonlinear relationship</abbr> where better credit worthiness leads to lower interest rates, plus some random noise.</p>
<pre class="r"><code>interest_rate = 0.05 + 0.03 * exp(-credit_worthiness) + rnorm(n, 0, 0.01)</code></pre>
<pre><code>## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.4     ✔ readr     2.1.5
## ✔ forcats   1.0.0     ✔ stringr   1.5.1
## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
## ✔ purrr     1.0.2     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-2.png" width="672" /><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
<pre><code>## [1] &quot;Correlation Matrix:&quot;</code></pre>
<pre><code>##                   credit_worthiness interest_rate default_prob
## credit_worthiness             1.000        -0.764       -0.863
## interest_rate                -0.764         1.000        0.806
## default_prob                 -0.863         0.806        1.000</code></pre>
<pre class="r"><code>genEndogData &lt;- function(n=100){
  sigma &lt;- matrix(c(40,12,12,20), ncol=2)
  de &lt;- mvtnorm::rmvnorm(n=n, mean=c(0,0), sigma=sigma)
  Z &lt;- seq(1,5,length.out = n)
  X &lt;- 3*Z + de[,1]
  Y &lt;- 2*X + de[,2] 
  data.frame(Y,X,Z)
}

test_data &lt;- genEndogData()

test_model &lt;- glm(Z ~ X + Y,
                  family = &quot;gaussian&quot;,
                  data = test_data)

viz_test_data &lt;- test_data %&gt;%
  mutate(
    predicted_prob = predict(test_model, type = &quot;response&quot;),
    X_group = cut(X, 
                                breaks = quantile(X, probs = 0:5/5),
                                labels = c(&quot;Very Low&quot;, &quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;, &quot;Very High&quot;))
  )

# Plot relationships
ggplot(viz_test_data, aes(x = Y, y = Z)) +
  geom_point(alpha = 0.3, aes(color = X_group)) +
  geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;gaussian&quot;)) +
  facet_wrap(~X_group) +
  labs(title = &quot;(target: Z) vs (value: Y)&quot;,
       subtitle = &quot;Grouped by: X&quot;,
       x = &quot;Y (%)&quot;,
       y = &quot;Z&quot;,
       color = &quot;X&quot;) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>genEndogData &lt;- function(n=100){
  sigma &lt;- matrix(c(40,12,12,20), ncol=2)
  de &lt;- mvtnorm::rmvnorm(n=n, mean=c(0,0), sigma=sigma)
  Z &lt;- seq(1,5,length.out = n)
  X &lt;- 3*Z + de[,1]
  Y &lt;- 2*X + de[,2] 
  data.frame(Y,X,Z)
}

test_data &lt;- genEndogData()

test_model &lt;- glm(Z ~ X + Y,
                  family = &quot;gaussian&quot;,
                  data = test_data)

viz_test_data &lt;- test_data %&gt;%
  mutate(
    predicted_prob = predict(test_model, type = &quot;response&quot;),
    Y_group = cut(Y, 
                                breaks = quantile(Y, probs = 0:5/5),
                                labels = c(&quot;Very Low&quot;, &quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;, &quot;Very High&quot;))
  )

# Plot relationships
ggplot(viz_test_data, aes(x = X, y = Z)) +
  geom_point(alpha = 0.3, aes(color = Y_group)) +
  geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;gaussian&quot;)) +
  facet_wrap(~Y_group) +
  labs(title = &quot;(target: Z) vs (value: X)&quot;,
       subtitle = &quot;Grouped by: Y&quot;,
       x = &quot;X (%)&quot;,
       y = &quot;Z&quot;,
       color = &quot;Y&quot;) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># Fit naive model
naive_model &lt;- glm(default ~ interest_rate + business_age + log(annual_revenue),
                  family = binomial(link = &quot;logit&quot;),
                  data = loan_data)

# Create visualization data
viz_data &lt;- loan_data %&gt;%
  mutate(
    predicted_prob = predict(naive_model, type = &quot;response&quot;),
    creditworthiness_group = cut(creditworthiness, 
                                breaks = quantile(creditworthiness, probs = 0:5/5),
                                labels = c(&quot;Very Low&quot;, &quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;, &quot;Very High&quot;))
  )

# Plot relationships
ggplot(viz_data, aes(x = interest_rate, y = default)) +
  geom_point(alpha = 0.3, aes(color = creditworthiness_group)) +
  geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)) +
  facet_wrap(~creditworthiness_group) +
  labs(title = &quot;Default Probability vs Interest Rate&quot;,
       subtitle = &quot;Grouped by Creditworthiness&quot;,
       x = &quot;Interest Rate (%)&quot;,
       y = &quot;Default Probability&quot;,
       color = &quot;Creditworthiness&quot;) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Simulate instruments
simulate_instruments &lt;- function(loan_data, n_samples = nrow(loan_data)) {
  # Federal funds rate (varies by time)
  fed_rate &lt;- rep(runif(n_samples/20, 1, 3), each = 20) + 
              rnorm(n_samples, 0, 0.1)
  
  # Bank liquidity (varies by region)
  bank_liquidity &lt;- rep(rnorm(n_samples/10), each = 10)
  
  # Create instrument dataset
  data.frame(
    fed_rate = fed_rate,
    bank_liquidity = bank_liquidity,
    # Interest rate spread (market-wide)
    market_spread = 2 + 0.5 * fed_rate + rnorm(n_samples, 0, 0.2)
  )
}

# Add instruments to our data
loan_data &lt;- cbind(loan_data, simulate_instruments(loan_data))

# Two-Stage Least Squares (2SLS)
# First stage: predict interest rates using instruments
stage1 &lt;- lm(interest_rate ~ fed_rate + bank_liquidity + market_spread +
             business_age + log(annual_revenue),
             data = loan_data)

# Get predicted interest rates
loan_data$predicted_rate &lt;- predict(stage1)

# Second stage: use predicted rates in default model
stage2 &lt;- glm(default ~ predicted_rate + business_age + log(annual_revenue),
              family = binomial(link = &quot;logit&quot;),
              data = loan_data)

# Compare coefficients
models_comparison &lt;- data.frame(
  Variable = names(coef(naive_model)),
  Naive_Estimate = coef(naive_model),
  IV_Estimate = c(coef(stage2)[1], 
                 coef(stage2)[&quot;predicted_rate&quot;],
                 coef(stage2)[c(&quot;business_age&quot;, &quot;log(annual_revenue)&quot;)])
)</code></pre>
<pre class="r"><code># Function to evaluate model performance
evaluate_model &lt;- function(model, data, predicted_rate = FALSE) {
  # Get predictions
  if(predicted_rate) {
    data$interest_rate &lt;- data$predicted_rate
  }
  predictions &lt;- predict(model, newdata = data, type = &quot;response&quot;)
  
  # Calculate metrics
  auc &lt;- pROC::auc(data$default, predictions)
  
  # Calculate calibration metrics
  calibration &lt;- data.frame(
    pred_prob = predictions,
    actual = data$default
  ) %&gt;%
    mutate(bucket = cut(pred_prob, breaks = seq(0, 1, 0.1))) %&gt;%
    group_by(bucket) %&gt;%
    summarise(
      n = n(),
      pred = mean(pred_prob),
      actual = mean(actual)
    )
  
  list(auc = auc, calibration = calibration)
}

# Compare models
naive_perf &lt;- evaluate_model(naive_model, loan_data)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>iv_perf &lt;- evaluate_model(stage2, loan_data, predicted_rate = TRUE)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>naive_perf</code></pre>
<pre><code>## $auc
## Area under the curve: 0.722
## 
## $calibration
## # A tibble: 5 × 4
##   bucket        n   pred actual
##   &lt;fct&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 (0,0.1]     691 0.0488 0.0492
## 2 (0.1,0.2]   235 0.138  0.128 
## 3 (0.2,0.3]    57 0.243  0.263 
## 4 (0.3,0.4]    15 0.340  0.4   
## 5 (0.4,0.5]     2 0.463  0.5</code></pre>
<pre class="r"><code>iv_perf</code></pre>
<pre><code>## $auc
## Area under the curve: 0.5416
## 
## $calibration
## # A tibble: 2 × 4
##   bucket        n   pred actual
##   &lt;fct&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 (0,0.1]     840 0.0817 0.0845
## 2 (0.1,0.2]   160 0.109  0.0938</code></pre>
</div>
</div>
