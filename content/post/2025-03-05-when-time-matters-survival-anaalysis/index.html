---
title: 'When Time Matters: Survival Anaalysis'
author: Youbeen Shim
date: '2025-03-05'
slug: when-time-matters-survival-anaalysis
categories: []
tags: []
---



<p>Imagine. You are working at Candy Emporium, a subscription business that delivers sweets to customers monthly. A key risk in your business is customer churn - losing your valuable customers. Some customers have been with your company for years, others for just weeks. Some have already canceled, while others are still active. How do you make sense of this messy, incomplete picture to predict who might leave next? Perhaps, you could even understand what expensive mistakes you are making that are costing you customers.
Having dipped my toes in both academia and industry, I frequently notice how survival analysis remains underutilized despite its practical value. Many data scientists are quick to reach for regression or classification techniques, but when time itself becomes a critical factor in your analysis, survival methods provide insights that other approaches simply cannot.</p>
<p>Today, we will cover the three most common approaches in survival analysis, namely: Kaplan-Meier method, log rank test, and Cox’s proportional-hazards model.</p>
<div id="motivation-why-is-survival-analysis-useful" class="section level2">
<h2>Motivation: Why is survival analysis useful?</h2>
<p>In many scenarios, the main outcome that is being assessed is the time to an event of interest. The generic name for this is <em>survival time</em>, which can be confusing since time <em>survived</em> could just as easily apply to time taken for a customer to churn as it could to time from diagnosis of a disease to death.</p>
<p>Of course, linear models are very powerful, and one could argue that you could use the simpler multivariate linear model with <em>survival_time</em> as the dependent variable to conduct a similar study. However, data collection processes are commonly conducted a finite number of times on a fixed interval, and it is typical for some of the individuals to not have experienced the event of interest at the end. Moreover, survival data are rarely normally distributed and often skewed, compromising usually of many early events and relatively few late ones. <abbr title="that happens more often you might think">These scenarios</abbr> make survival analysis particularly attractive.</p>
</div>
<div id="core-concepts" class="section level2">
<h2>Core concepts</h2>
<div id="survival-function" class="section level3">
<h3>Survival function</h3>
<p>The survival probability (also called the survivor function) <span class="math inline">\(S(t)\)</span> is the probability that an individual survives beyond time <span class="math inline">\(t\)</span>. Mathematically, if <span class="math inline">\(T\)</span> is the survival time (which is a random variable), then:</p>
<p><span class="math display">\[S(t) = P(T &gt; t)\]</span></p>
<p>In plain language, <span class="math inline">\(S(t)\)</span> tells us what fraction of our population is still “<em>alive</em>” (hasn’t experienced the event) at time <span class="math inline">\(t\)</span>. This function always starts at <span class="math inline">\(1\)</span> (everyone is alive at the beginning) and decreases toward <span class="math inline">\(0\)</span> as time goes on.</p>
<p>But what makes working with <span class="math inline">\(S(t)\)</span> tricky?</p>
</div>
<div id="censoring" class="section level3">
<h3>Censoring</h3>
<p>The biggest challenge is that most of the time, only some individuals have experienced the event and, subsequently, survival times will be unknown for the subset of the study group. This phenomenon is called censoring, and while there is a distinction between <abbr title="event, assuming it were to occur, happens after a certain specific time">right</abbr>, <abbr title="the event happened prior to a certain specific time">left</abbr>, and <abbr title="the event happened in between two specific intervals">interval</abbr> censoring, we will focus on right censoring for simplification.</p>
<p>For example, in a 5-year study of cancer recurrence:</p>
<ul>
<li>If a patient has a recurrence at year 3, we know their exact survival time (3 years)</li>
<li>If a patient drops out of the study after 2 years with no recurrence, they are “censored” at 2 years</li>
<li>If a patient completes the study with no recurrence, they are censored at 5 years</li>
</ul>
<p>As the observations are incomplete, it may be intuitive to discard them as we move forward with the analysis. However, this would not only waste valuable information, but also likely bias our results.</p>
<p>Instead, we change the question from “did the event happen?” to <abbr title="more strictly, the question would be: what is the risk of the event happening at each point in time, given survival up to that point?">“what is the risk of the event happening, given that it has not happened yet?”</abbr>.</p>
<p>This brings us to our third and final key concept.</p>
</div>
<div id="hazard-function" class="section level3">
<h3>Hazard function</h3>
<p>The hazard, denoted by <span class="math inline">\(h(t)\)</span> or <span class="math inline">\(\lambda(t)\)</span>, is the probability that an individual who is under observation at a time <span class="math inline">\(t\)</span> and has not experienced an event yet has an <abbr title="the so-called \“instantaneous risk\”">event at that time</abbr>. Mathematically:</p>
<p><span class="math display">\[h(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T &lt; t + \Delta t | T \geq t)}{\Delta t}\]</span></p>
<p>Which may look intimidating, but really just simply asks the following: if you’ve survived until time <span class="math inline">\(t\)</span>, what’s the probability you’ll experience the event in the next small time interval?</p>
<p>Note that, in contrast to the survivor function, which focuses on not having an event, the hazard function focuses on the event occurring.</p>
<p>What’s particularly useful about the hazard function is that it can take different shapes:</p>
<ul>
<li>Constant hazard (exponential distribution): the risk never changes</li>
<li>Increasing hazard (Weibull distribution): the risk grows over time (think aging)</li>
<li>Decreasing hazard: the risk diminishes over time</li>
<li>Bathtub-shaped hazard: high risk early, then low, then high again (think human mortality)</li>
</ul>
</div>
</div>
<div id="core-methods" class="section level2">
<h2>Core Methods</h2>
<p>Let’s return to where we started: Candy Emporium. You like your subscribers, you want to retain them, so you devised a strategy. A <abbr title="yes, this is not an ideal set-up for testing as it lacks control, but it's more straightforward.">portion</abbr> of your customers will receive a series of standard email campaigns; a separate portion of your customers will receive a personalized recommendation in the form of surprise gifts. Which strategy is more effective? Let’s set up.</p>
<details>
<summary>
click here to for full code to generate sample data
</summary>
<p>We will simulate 200 customers, 100 for each retention strategy. Remember that:</p>
<ul>
<li>Strategy A: Standard email campaigns</li>
<li>Strategy B: Personalized recommendations + surprise gifts</li>
</ul>
<pre class="r"><code>library(dplyr)
set.seed(1008)

# Create subscription duration data (in months)
subscription_data &lt;- data.frame(
  customer_id = 1:200,
  strategy = rep(c(&quot;A&quot;, &quot;B&quot;), each = 100),
  subscription_months = c(
    rexp(100, rate = 1/8), # assuming that more customers will churn earlier
    rexp(100, rate = 1/12) # designating this as the &quot;better&quot; retention strategy
  ),
  # censoring indicator (in this case, implying that the customers are still active/retained)
  status = sample(c(1, 0), 200, replace = TRUE, prob = c(0.7, 0.3)) # assuming that ~30% of customers to still be active 
) %&gt;%
  mutate(subscription_months = pmax(round(subscription_months), 1)) # rounding months, but ensuring that it is at least 1</code></pre>
</details>
<pre><code>##   customer_id strategy subscription_months status
## 1           1        A                   3      1
## 2           2        A                   3      0
## 3           3        A                   5      1
## 4           4        A                   1      0
## 5           5        A                   4      0
## 6           6        A                   2      1</code></pre>
<pre><code>##     customer_id strategy subscription_months status
## 195         195        B                   1      0
## 196         196        B                  39      0
## 197         197        B                  22      1
## 198         198        B                   3      1
## 199         199        B                   7      1
## 200         200        B                   5      1</code></pre>
<div id="seeing-survival-kaplan-meier-method" class="section level3">
<h3>Seeing Survival: Kaplan-Meier Method</h3>
<p>The Kaplan-Meier method, commonly abbreviated as KM method, provides us with a way to estimate the survival function using the observed data. It is particularly useful due to its nonparametric quality, which does not require assumptions about the underlying distribution.</p>
<p>The core principle that drives this method is the fact that the probability of surviving paste a certain point is the product of the conditional probabilities of surviving each preceding time point.</p>
<p>For each time point <span class="math inline">\(t_i\)</span> where at least one event occurs:</p>
<ol style="list-style-type: decimal">
<li>Count how many individuals are still “at risk” (<span class="math inline">\(n_i\)</span>)</li>
<li>Count how many events occur at that time (<span class="math inline">\(d_i\)</span>)</li>
<li>Calculate the conditional probability of surviving: <span class="math inline">\(p_i = (n_i - d_i)/n_i\)</span></li>
<li>Update the cumulative survival probability: <span class="math inline">\(S(t_i) = S(t_{i-1}) \times p_i\)</span></li>
</ol>
<p>Once the data is appropriately prepared (in our case, it was designed in such a way), it is relatively simple to fit the Kaplan-Meier survival curves.</p>
<pre class="r"><code>library(survival)
library(ggplot2)
library(ggsurvfit)

km_fit &lt;- survfit(Surv(subscription_months, status) ~ strategy, 
                  data = subscription_data)</code></pre>
<p>Which can then be printed in a table form using the summary() function, or</p>
<pre class="r"><code>summary(km_fit)</code></pre>
<p>more commonly, in a chart represented as a curve.</p>
<pre class="r"><code>ggsurvfit(km_fit) +
  labs(
    title = &quot;Customer Retention by Strategy&quot;,
    x = &quot;Months Since Subscription Start&quot;,
    y = &quot;Retention Probability&quot;
  ) +
  add_confidence_interval() +
  add_risktable() +
  scale_color_discrete(
    name = &quot;Strategy&quot;,
    labels = c(&quot;A: Standard&quot;, &quot;B: Personalized&quot;)
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>A simple visual inspection of the Kaplan-Meier curve provides several insights:</p>
<ul>
<li>Median survival time: The point at which the curve crosses the 0.5 probability line (on the y-axis)</li>
<li>Shape of decline: Gradual? Steep? At which point in time?</li>
<li>(Potential for) long-term survival: Is there a point where the <abbr title="this suggests that some patients may never experience the event">curve plateaus</abbr>?</li>
<li>Comparison between groups: By visually inspecting the KM curves of different groups</li>
</ul>
</div>
<div id="testing-the-difference-log-rank-test" class="section level3">
<h3>Testing the Difference: Log Rank Test</h3>
<p>Say that we’ve plotted the Kaplan-Meier curves, and there is (at least visually) a pretty big gap between the two curves. How do we know if the differences are statistically significant?
The log-rank test is the standard non-parametric approach for comparing survival curves between groups. Methodologically, the test answers the question:</p>
<blockquote>
<p>At each event time, is the number of events in each group what we would expect if the groups had identical risk?</p>
</blockquote>
<p>At a high level, we follow the procedure below:</p>
<ol start="0" style="list-style-type: decimal">
<li>Null hypothesis: there is no difference in survival between groups</li>
<li>Look at each time point where an event occurs</li>
<li>Compare the observed number of events in each group to what would be expected if the groups had equal risk</li>
<li>Combine these comparisons across all time points, creating our test statistic</li>
<li>Return a p-value, which examines if the differences are likely due to chance</li>
</ol>
<p>In R, again, it ends up being pretty straight forward:</p>
<pre class="r"><code># log-rank test: note the survdiff() function in stead of survfit() function for KM
log_rank &lt;- survdiff(Surv(subscription_months, status) ~ strategy, 
                    data = subscription_data)
log_rank</code></pre>
<pre><code>## Call:
## survdiff(formula = Surv(subscription_months, status) ~ strategy, 
##     data = subscription_data)
## 
##              N Observed Expected (O-E)^2/E (O-E)^2/V
## strategy=A 100       64     57.5     0.745      1.42
## strategy=B 100       78     84.5     0.506      1.42
## 
##  Chisq= 1.4  on 1 degrees of freedom, p= 0.2</code></pre>
<p>The Log-Rank Statistic follows a chi-square distribution with degrees of freedom equal to the number of groups minus 1 (so 1 in our case here).</p>
<pre class="r"><code>p_value &lt;- 1 - pchisq(log_rank$chisq, df = 1)
p_value</code></pre>
<pre><code>## [1] 0.2328082</code></pre>
<p>In our example, our p-value indicates that there is not enough evidence of a difference in retention between strategies (Bummer).</p>
</div>
<div id="handling-multiple-variables-coxs-proportional-hazards-model" class="section level3">
<h3>Handling Multiple Variables: Cox’s Proportional Hazards Model</h3>
<p>So far, we visualized and compared survival curves, but how do we <abbr title="an extremely common need in research">adjust for other factors</abbr> that might influence survival?
Analogous to a multiple regression model, Cox’s proportional hazards model enables the difference between survival times of particular groups to be tested while quantifying and adjusting for other covariates.</p>
<p>Note: Being <em>semi</em>-parametric, the Cox model does not require assumptions regarding the shape of the baseline hazard function. However, the model does assume that covariates multiply the hazard by specific, consistent, factors.</p>
<p>The model is expressed as:</p>
<p><span class="math display">\[h(t|X) = h_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p)\]</span>
Where:</p>
<ul>
<li><span class="math inline">\(h(t|X)\)</span> is the hazard at time <span class="math inline">\(t\)</span> for an individual with covariate values <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(h_0(t)\)</span> is the baseline hazard (left unspecified)</li>
<li><span class="math inline">\(\beta_i\)</span> are the coefficients we estimate</li>
<li><span class="math inline">\(X_i\)</span> are our covariates (age, treatment, etc.)</li>
</ul>
<p>The exponentiated coefficients, <span class="math inline">\(\exp(\beta_i)\)</span>, give us <strong>hazard ratios</strong> – the multiplicative effect on the hazard for a one-unit increase in the corresponding covariate.</p>
<p>Returning to our example, we can expand it by including customer characteristics as covariates. Let’s see… <abbr title="monthly subscription price tier, higher tier being more premium"><em>price_tier</em></abbr>, <abbr title="customer age group. something like 1) student 2) young adult 3) working adult 4) retired adult"><em>age_group</em></abbr>, <abbr title="whether they had subscription services offered by our company before"><em>prior_subscriptions</em></abbr>, and <abbr title="whether they were referred"><em>referred</em></abbr> seem like reasonable, realistic covariates.</p>
<pre class="r"><code>subscription_data &lt;- subscription_data %&gt;%
  mutate(
    price_tier = sample(1:3, 200, replace = TRUE, prob = c(0.3, 0.5, 0.2)),
    age_group = sample(1:4, 200, replace = TRUE),
    prior_subscriptions = rpois(200, lambda = 2),
    referred = sample(c(0, 1), 200, replace = TRUE, prob = c(0.7, 0.3))
  )</code></pre>
<p>Which we can then plug in the model.</p>
<pre class="r"><code>cox_model &lt;- coxph(
  Surv(subscription_months, status) ~ strategy + price_tier + 
        age_group + prior_subscriptions + referred, 
  data = subscription_data
)

summary(cox_model)</code></pre>
<pre><code>## Call:
## coxph(formula = Surv(subscription_months, status) ~ strategy + 
##     price_tier + age_group + prior_subscriptions + referred, 
##     data = subscription_data)
## 
##   n= 200, number of events= 142 
## 
##                         coef exp(coef) se(coef)      z Pr(&gt;|z|)
## strategyB           -0.22072   0.80194  0.17457 -1.264    0.206
## price_tier          -0.02626   0.97408  0.12267 -0.214    0.830
## age_group           -0.04397   0.95699  0.07482 -0.588    0.557
## prior_subscriptions -0.03474   0.96585  0.06636 -0.524    0.601
## referred            -0.06890   0.93342  0.18390 -0.375    0.708
## 
##                     exp(coef) exp(-coef) lower .95 upper .95
## strategyB              0.8019      1.247    0.5696     1.129
## price_tier             0.9741      1.027    0.7659     1.239
## age_group              0.9570      1.045    0.8265     1.108
## prior_subscriptions    0.9659      1.035    0.8481     1.100
## referred               0.9334      1.071    0.6509     1.339
## 
## Concordance= 0.551  (se = 0.029 )
## Likelihood ratio test= 2.28  on 5 df,   p=0.8
## Wald test            = 2.29  on 5 df,   p=0.8
## Score (logrank) test = 2.29  on 5 df,   p=0.8</code></pre>
<p>Notice how the summary function conveniently exponentiates the coefficients for us in the second table above.</p>
<p><abbr title="assuming that the p-value fell below our previously determined level">How do we interpret the results?</abbr></p>
<p>First, we see that Strategy B (personalized recommendations + surprise gifts) has a hazard ratio of 0.801 compared to Strategy A. This means that Strategy B reduces churn risk by approximately 20% (1 - .801) when controlling for other factors.</p>
<p>Similarly, the hazard ratio of <em>referred</em> is 0.933, meaning that the customers who were referred by friends have approximately 7% lower churn risk than those who found our services by themselves. If, for instance, the hazard ratio of <em>prior_subscriptions</em> was 1.080, it would mean that each additional prior subscription that a customer had increases their risk of churn by 8%.</p>
</div>
</div>
<div id="typical-ways-that-assumptions-are-violated" class="section level2">
<h2>Typical Ways that Assumptions are Violated</h2>
<p>As with any statistical method, survival analysis has its limitations and potential pitfalls.</p>
<div id="proportional-hazards" class="section level3">
<h3>Proportional Hazards</h3>
<p>While the Kaplan-Meier method does not, both the log rank test and <abbr title="which literally contains the assumption in the name">Cox’s proportional hazards model</abbr> require that the effect of each covariate is constant over time.</p>
<p>In other words, if the hypothetical Treatment X cuts the risk of a patient’s death by half at month 1, it should still cut the risk in half at month 12, 18, or 108.</p>
<p>Checking this assumption, thankfully, is fairly straightforward. Note that the assumption of proportional hazards is equivalent to assuming that the difference between the logarithms of the hazards for the two treatments does not change with time. This, in turn, is equivalent to saying that the difference between the logarithms of the cumulative hazard functions is constant.</p>
<p>In R, the function cox.zph() uses <abbr title="the difference between the observed covariate values and their expected values">Schoenfield residuals</abbr> to test for proportional hazards. Intuitively, if the effect of the covarite is constant over time, the plot of the residuals across time should not have a pattern or trend. Practically, you’re looking for <abbr title="a systematic pattern (like an upward or downward trend) suggests the effect of that variable changes over time">random scatter</abbr> around a horizontal line at y=0.</p>
<pre class="r"><code>ph_test &lt;- cox.zph(cox_model)

par(mfrow = c(2, 2))
plot(ph_test[1], main=&quot;Strategy&quot;) 
plot(ph_test[2], main=&quot;Price Tier&quot;)
plot(ph_test[4], main=&quot;Prior Subscriptions&quot;)  
plot(ph_test[5], main=&quot;Referred&quot;)  </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>In the above figure, the lines are roughly parallel, suggesting that the proportional hazards assumption is reasonable.</p>
<p>Another easy test is to simply check the p-values.</p>
<pre class="r"><code>ph_test</code></pre>
<pre><code>##                      chisq df    p
## strategy            0.0524  1 0.82
## price_tier          0.7268  1 0.39
## age_group           0.4771  1 0.49
## prior_subscriptions 0.7865  1 0.38
## referred            0.4036  1 0.53
## GLOBAL              2.0931  5 0.84</code></pre>
<p>A p-value that is greater than 0.05 implies that the assumption is met.</p>
<p>If the proportional hazards assumption does not hold, other than considering alternative models, you could 1) <abbr title="note: prevents estimating hazard ratios for the stratified variable">stratify by the problematic variable</abbr> 2) <abbr title="note: may be computationally intensive for large datasets">add time-dependent coefficients</abbr>, or 3) split the follow-up time into intervals that do follow proportional hazards. Dive deeper <a href="https://www.bookdown.org/rwnahhas/RMPH/survival-phassumption.html">here</a>.</p>
</div>
<div id="competing-risks" class="section level3">
<h3>Competing Risks</h3>
<p>What if we are studying cancer patient’s survival times, and the patient dies from a traffic accident? Such scenarios that prevent us from observing the event of interest, while uncommon, are very real risks.</p>
<p>Typically, these are treated similarly to censored events, but this can lead to biased results.</p>
</div>
<div id="informative-censoring" class="section level3">
<h3>Informative Censoring</h3>
<p>Key assumption in survival analysis is non-informative censoring. The fact that a client or a patient has dropped out should not give any information about the individual’s hazard, and thus their risk should be the same as those who remain in the study.</p>
<p>This assumption is often violated in practice. Patients who discontinue a trial, perhaps due to side effects they have been experiencing, may have different underlying risk profiles. Sensitivity analysis should be considered and conducted in order to assess the impact of potentially informative censoring.</p>
</div>
<div id="time-varying-covariates" class="section level3">
<h3>Time-Varying Covariates</h3>
<p>Drug resistance, treatment exposures, and biomarker levels are examples of covariates whose values may change over time during the follow-up period. While the Cox model can be <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6015946/">extended</a> to handle such variables, it requires specialized approach and a more careful interpretation.</p>
</div>
</div>
<div id="next-steps" class="section level2">
<h2>Next Steps</h2>
<p>The methods that I have covered today focus primarily on baseline assumptions and common scenarios. However, survival analysis has since only grown, and there are both methodologies and models that extend beyond these fundamentals. If you are interested, or if the assumptions go beyond what we covered today, consider exploring the following:</p>
<div id="parametric-models" class="section level3">
<h3>Parametric Models</h3>
<p>As we’ve established before, Cox’s proportional hazards model is semi-parametric. Parametric models, which depend on more assumptions but provide a more precise prediction, specify the baseline hazard function utilizing distributions such as:</p>
<ul>
<li>Weibull for increasing or decreasing hazards;</li>
<li>Exponential for constant hazards; and</li>
<li>Log-normal or log-logistic for non-monotonic hazards</li>
</ul>
</div>
<div id="random-forest-approach" class="section level3">
<h3>Random Forest approach</h3>
<p>Machine learning approaches such as <a href="https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2020.551420/full">Random Survival Forest</a> can capture complex non-linear relationships and interactions between covariates without requiring the proportional hazards assumption. In my experience it is <a href="https://pubmed.ncbi.nlm.nih.gov/36169048/">more computationally complex and more challenging to interpret</a>, but shines when there are more events and samples.</p>
</div>
<div id="joint-models" class="section level3">
<h3>Joint Models</h3>
<p>There is a subclass of “joint” models that attempt to account for the complex <strong>relationship</strong> between longitudinal biomarker trajectories and survival outcomes, providing more accurate and comprehensive analyses in clinical research settings. They include <a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300112">Flexible Parametric Joint Models</a> and <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4503792/">Shared Random Effects Models</a>, but require a thorough consideration of the longitudinal data before implementation.</p>
</div>
</div>
<div id="challenge" class="section level2">
<h2>Challenge</h2>
<p>Whether it be patient outcomes, customer churn, equipment failure, or any process where timing matters, survival analysis helps to answer not just “if” but “when” events occur and what factors influence their timing.</p>
<p>Introducing survival analysis to clients often transforms their understanding of their data. The ability to visualize survival curves, compare them with mathematical and methodological rigor, and model the effects of different variables opens new avenues for decision-making and research.</p>
<p>Think about this as you go forward: How can you reframe your current objectives to be time-to-event oriented? Do you encounter censorship in your data? What decisions can you help drive by quantifying survival differences between 2+ groups? And, as always, please let me know how it goes.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Bewick, V., Cheek, L., &amp; Ball, J. (2004). Statistics review 12: Survival analysis. Critical Care, 8(5), 389-394. <a href="https://doi.org/10.1186/cc2955" class="uri">https://doi.org/10.1186/cc2955</a></p>
<p>Clark, T. G., Bradburn, M. J., Love, S. B., &amp; Altman, D. G. (2003). Survival Analysis Part I: Basic concepts and first analyses. British Journal of Cancer, 89(2), 232-238. <a href="https://doi.org/10.1038/sj.bjc.6601118" class="uri">https://doi.org/10.1038/sj.bjc.6601118</a></p>
<p>Jenkins, S. P. (2005). Survival Analysis. Unpublished Manuscript, Institute for Social and Economic Research, University of Essex. Retrieved from <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=9bb46b98492c0d8e33ffbddab4a0f99d84f3f0c0" class="uri">https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=9bb46b98492c0d8e33ffbddab4a0f99d84f3f0c0</a></p>
<p>Kassambara, A., Kosinski, M., &amp; Biecek, P. (2017). Cox Proportional-Hazards Model. STHDA. Retrieved March 5, 2025, from <a href="https://www.sthda.com/english/wiki/cox-proportional-hazards-model" class="uri">https://www.sthda.com/english/wiki/cox-proportional-hazards-model</a></p>
</div>
